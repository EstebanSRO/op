# -*- coding: utf-8 -*-
"""proyecto_analiticas_2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kil4iXSUNxSUo2E-PIzrLJXFTrucFtmM
"""

pip install category_encoders

# Commented out IPython magic to ensure Python compatibility.
#Proyecto final analiticas 2
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from numpy import nan
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import category_encoders as ce
import re
import math
import itertools
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, accuracy_score
import re
import math
import itertools
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, accuracy_score
# %matplotlib notebook

pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)

df =  'https://drive.google.com/u/0/uc?id=1JiLgQkf1E9XAQGrn5hAJIb1Qa2Mt7fw_&export=download'
df= pd.read_excel(df, sheet_name='Exhibit 1 -- Data', engine='openpyxl')

"""# Esto ayuda asignar el ID como index"""

df.set_index('ID',inplace=True)

df.head()

"""# Identificacion de NaN en la base datos "5318 NaN"   """

df.isnull().sum().sort_values(ascending=False).sum()

nulls=df.isnull().sum().sort_values(ascending=False)
nulls[nulls>0]

"""# Cuantos Datos Me quedarian si uso un dropna()                                             "192 Observaciones" perderia el 92% de mis datos"""

print('"Antes"' ,len(df.index), 'Observaciones VS "Despues" ',df.dropna(axis=0).count()['Retained.in.2012.'], 'Obersavaciones' )

"""# Cuantos Nulls puedo quitar si hago un dropna(how = 'all'), "No existe ninguna observacion que todos sus datos sean NaN"
"""

print('"Antes" 2389 Observaciones VS "Despues"',df.dropna(how='all').count()['Retained.in.2012.'], 'Obersavaciones' )

"""# Dropear Variables Similares

SchoolGradeTypeHigh & SchoolGradeTypeLow se pueden sacar de la variable "SchoolGradeType"
"""

df.drop(['SchoolGradeTypeHigh','SchoolGradeTypeLow'], axis=1, inplace = True)

"""GroupGradeTypeLow & GroupGradeTypeHigh se pueden sacar de la variable "GroupGradeType"
"""

df.drop(['GroupGradeTypeLow','GroupGradeTypeHigh'], axis=1, inplace = True)

"""Se dropean esta variables debido a que variables como "Mes","DifferenceTraveltoFirstMeeting" y "DifferenceTraveltoFirstMeeting" cumplen mejor su funcion"""

df.drop(['LastMeeting','FirstMeeting','Return.Date','Early.RPL'],axis=1,inplace=True)

"""# Transformacion Variable

Se opto por esta combinacion debido a que Differece Travel con tiene Nulls
"""

df['Time_travel1']= df['Departure.Date']-df['Latest.RPL']

"""# Identificacion de NaN en la base datos "3990 NaN"
"""

df.isnull().sum().sort_values(ascending=False).sum()

nulls=df.isnull().sum().sort_values(ascending=False)
nulls[nulls>0]

"""# Retener o Eliminar "Special.Pay"? (1919 NaN)

Special.Pay es una variable categorica por lo que seria necesario hacer un Naive Bayes para pronosticar
"""

df['Special.Pay'].unique()

"""Se contaria solo con 470 Datos de entrenamiento para pronosticar 1919 Datos por lo que correriamos el riesgo de clasificar erroneamente estos datos. No se recomienda hacer esto, debido a que no tenemos las observaciones suficientes para pronosticarlo"""

len(df.index)-nulls['Special.Pay']

df.drop('Special.Pay', axis=1, inplace= True)

"""# Identificacion de NaN en la base datos "2071 NaN"
"""

df.isnull().sum().sort_values(ascending=False).sum()

nulls=df.isnull().sum().sort_values(ascending=False)
nulls[nulls>0]

"""# Retener o Eliminar "Poverty.Code"? "599 NaN"

Poverty.Code es una variable categorica por lo que seria necesario hacer un Naive Bayes para pronosticar
"""

df['Poverty.Code'].unique()

"""Se propuso usar la variable "Income.Level" como proxi"""

df.drop('Poverty.Code',axis=1,inplace=True)

"""# Identificacion de NaN en la base datos "1472 NaN"
"""

df.isnull().sum().sort_values(ascending=False).sum()

nulls=df.isnull().sum().sort_values(ascending=False)
nulls[nulls>0]

"""# Retener o Eliminar "DifferenceTraveltoLastMeeting" & "DifferenceTraveltoFirstMeeting"? (337 NaN)

Dado que no tuvieron una cita los padres para ver temas del viaje, NumberOfMeetingswithParents  = 0  (esto explica los 337 Casos NaN)
"""

finding=df[df['NumberOfMeetingswithParents']==0][['NumberOfMeetingswithParents','DifferenceTraveltoLastMeeting','DifferenceTraveltoFirstMeeting']]
finding.head(5)

"""No es posible rellenar estos valores debido a que 'NumberOfMeetingswithParents' = 0"""

null=finding.isnull().sum()
null[null>0].sort_index()

df.drop(['DifferenceTraveltoFirstMeeting','DifferenceTraveltoLastMeeting'],axis=1,inplace=True)

"""# Identificacion de NaN en la base datos "798 NaN"
"""

df.isnull().sum().sort_values(ascending=False).sum()

nulls=df.isnull().sum().sort_values(ascending=False)
nulls[nulls>0]

"""# Rellenando La Variable "To.Grade"   (150 NaN)

Para rellenar algunos vacios, se propuso usar los valores de "MDR.High.Grade" en los vacios de "To.Grade", debido a que esta cuenta con informacion approximada del grado escolar
"""

IDs = df[df['To.Grade'].isnull()].index    #Identificacion de los IDs que tienen este campo vacio
df.loc[IDs,'To.Grade'] = df.loc[IDs,'MDR.High.Grade'] #Relleno de la informacion a los IDs Mappeados

print(df['To.Grade'].isnull().sum(),'NaN en la variable To.Grade')

df.drop('MDR.High.Grade',axis=1 , inplace = True)

nulls=df.isnull().sum().sort_values(ascending=False)
nulls[nulls>0]

"""# Dropeo From.Grade

Se decide tirar debido a su poca correlacion y significancia
"""

df.drop('From.Grade',axis=1 , inplace = True)

"""# Identificacion de NaN en la base datos "467 NaN"
"""

df.isnull().sum().sort_values(ascending=False).sum()

nulls=df.isnull().sum().sort_values(ascending=False)
nulls[nulls>0]

"""# Dropear las variables NaN VS Tirar los IDs NaN

Dropear los IDs
"""

print('Varianza Antes',np.var(df).sum(),'VS Varianza Despues',np.var(df.dropna()).sum())
print('Estoy Perdiendo',(np.var(df.dropna()).sum()-np.var(df).sum())/(np.var(df).sum()*100))

"""Dropeando Las Columnas"""

print('Varianza Antes',np.var(df).sum(),'VS Varianza Despues',np.var(df.dropna(axis=1)).sum())
print('Estoy Perdiendo',(np.var(df.dropna(axis=1)).sum()-np.var(df).sum())/(np.var(df).sum()*100))

df.dropna(inplace=True)

"""# Identificacion de NaN en la base datos "0 NaN"
"""

df.isnull().sum().sort_values(ascending=False).sum()

nulls=df.isnull().sum().sort_values(ascending=False)
nulls[nulls>0]

"""###########Investigacion de la variable con missing value  ########################"""

fig, axs = plt.subplots(nrows=2, ncols=5, figsize=(8,8))
Columns = df.columns.to_list()
#para las variables que son de multiples segmentos si buscan llenarlas tendiamos que procedes con un naive bayes para llenarlas
#ejemplo proverty_code, schoolsizeindicator (esta se pudiera sacar con numero de alumnos en el viaje)

sns.histplot(data=df, x = Columns[0], hue='Retained.in.2012.', ax=axs[0,0])
sns.histplot(data=df, x = Columns[1], hue='Retained.in.2012.', ax=axs[0,1])
sns.histplot(data=df, x = Columns[2], hue='Retained.in.2012.', ax=axs[0,2])
sns.histplot(data=df, x = Columns[3], hue='Retained.in.2012.', ax=axs[0,3])

sns.histplot(data=df, x = Columns[4], hue='Retained.in.2012.', ax=axs[0,4])
sns.histplot(data=df, x = Columns[5], hue='Retained.in.2012.', ax=axs[1,0])
sns.histplot(data=df, x = Columns[6], hue='Retained.in.2012.', ax=axs[1,1])
sns.histplot(data=df, x = Columns[7], hue='Retained.in.2012.', ax=axs[1,2])

sns.histplot(data=df, x = Columns[8], hue='Retained.in.2012.', ax=axs[1,3])
sns.histplot(data=df, x = Columns[9], hue='Retained.in.2012.', ax=axs[1,4])

"""# Justificacion Del Modelo"""

df.drop('Program.Code',axis=1,inplace=True) # Existe una variable Mejor
df.drop('Group.State',axis=1,inplace=True) # Existe una variable Mejor llamada Region y cumple una mejor funcion
df.drop('Deposit.Date',axis=1,inplace=True) # Correlacion muy baja, no se le encuentra un proposito para predecir, no diferencia en media
df.drop('FRP.Cancelled',axis=1,inplace=True) # Correlacion muy baja, es muy similar al FRP.Active
df.drop('FRP.Take.up.percent.',axis=1,inplace=True) # Correlacion muy baja, no tiene mucho sentido
df.drop('Cancelled.Pax',axis=1,inplace=True) # Correlacion muy baja, no tiene mucho sentido
df.drop('Initial.System.Date',axis=1,inplace=True) #Initial System no va debido a su baja correlacion y es mas relevante hacer una diferencias con el departure
df.drop('CRM.Segment',axis=1,inplace=True) #CRM.Segment la retiramos debido a que no hay suficiente informacion y su correlacion es muy baja, ademas se encuentra con mejores variables tales como school type
df.drop('Parent.Meeting.Flag',axis=1,inplace=True) #Parent meeting flag, se va por que nos quedamos con el numero de sesiones con los padres
df.drop('MDR.Low.Grade',axis=1,inplace=True)#MDR.LOWGRADE NO ES SIGNIFICATIVA POR CORRELACION, NO OBSERVA UNA BUENA SEGMENTACION Y DEPENDE DE OTRAS VARIABLES PARA LLEGAR A FUNCIONAR
df.drop('Total.School.Enrollment',axis=1,inplace=True)# Baja correlacion, no hace sentido la distribucion
df.drop('EZ.Pay.Take.Up.Rate',axis=1,inplace=True) # el pago domiciliado no es un factor relevante que consideremeos
df.drop('Total.Pax',axis=1,inplace=True)# tiene multicolinealidad con la variabre discount pax y FPP
df.drop('SPR.Group.Revenue',axis=1,inplace=True)#SPR.Group.Revenue correlacion baja, no da sentido y dificil de explicar
df.drop('GroupGradeType',axis=1,inplace=True)#correlacion baja
df.drop('MajorProgramCode',axis=1,inplace=True)#MajorProgramCode correlacion baja, distribucion no se ve razonable
df.drop('FPP.to.School.enrollment',axis=1,inplace=True)#repetitivo a fpp quitar
df.drop('FPP.to.PAX',axis=1,inplace=True)#multi
df.drop('Num.of.Non_FPP.PAX',axis=1,inplace=True)#multi
df.drop('SchoolSizeIndicator',axis=1,inplace=True)#SchoolSizeIndicator quitar el total school enrollment es mejor
df.drop('Departure.Date',axis=1,inplace=True)#Se usa mejor time travel, mes o dias
df.drop('Latest.RPL',axis=1,inplace=True)#Se usa mejor time travel, mes o dias

df.head()

df.corr()['Retained.in.2012.'].sort_values(ascending=False)

#Program Code Dropear
#Tuiton > 2000  segmento se queda
#Travel.Type es una variable relevante segmenta bien en las formas de viaje
#Is.Non Annual se queda ya que segmenta bien la variable objetivo y tiene relevancia para identificar si viajan cada a;o
#FRP ACTIVE mantener tiene buena correlacion y motivo mantener el FRP
#Total.Discount.Pax relevante dado que el maestro es el que decide organizar el viaje, por lo que se asume que agarra un viaje con mayor beneficio tanto para el y sus alumnos
#School type se queda, se cree que tiene relevancia por las escuelas privadas
#Income se queda debido a que se considera muy importante en la decision
#School.Sponsor se queda buena correlacin buena distribucion
#SPR.PRODUCT.TYPE  SE QUEDA
#SPR.EXISTING  SE QUEDA SE CREE QUE SI A VIAJADO ANTES ES UN FACTOR CLAVE EN EL RETAINED
#FPP SE QUEDA
#SingleGradeTripFlag se mantiene excelente correlacion y buen significado en salir con personas del grado
#School grade type se queda
#DepartureMonth sequeda el mes es valioso en el analisis por temas climatologicos o vacaciones etc

df

X = pd.get_dummies(df.drop('Retained.in.2012.',axis=1),prefix='Dummie_')

y = df['Retained.in.2012.']

columns = list(X.columns)
results = pd.DataFrame(columns=['columns', 'accuracy'])

# Convertir 'Time_travel1' a un número entero que represente la cantidad de días
X['Time_travel1'] = X['Time_travel1'].dt.days

# Ahora puedes intentar entrenar el modelo nuevamente
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = LogisticRegression(max_iter=1000)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

print("Accuracy:", accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred))

from sklearn.preprocessing import StandardScaler

# Escalar los datos
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Dividir los datos en conjuntos de entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# Crear y entrenar el modelo de regresión logística
model = LogisticRegression(max_iter=1000)
model.fit(X_train, y_train)

# Predecir las etiquetas para el conjunto de prueba
y_pred = model.predict(X_test)

# Evaluar el modelo
print("Accuracy:", accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred))

for columna, tipo in df.dtypes.iteritems():
    print(f"Columna: {columna}, Tipo de dato: {tipo}")

# Convertir 'Time_travel1' a un número entero que represente la cantidad de días
df['Time_travel1'] = df['Time_travel1'].dt.days

# Realizar la codificación one-hot de las columnas de tipo 'object'
df = pd.get_dummies(df)

from sklearn.preprocessing import StandardScaler

# Crear y entrenar el modelo de regresión logística con todas las características
X = df.drop('Retained.in.2012.', axis=1)
y = df['Retained.in.2012.']

# Escalar los datos
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Crear y entrenar el modelo
model = LogisticRegression(max_iter=5000, solver='saga')
model.fit(X_scaled, y)

def preprocess(df):
    # Codificación de variables categóricas
    df = pd.get_dummies(df, columns=['Travel.Type', 'Region', 'School.Type', 'Income.Level', 'SPR.Product.Type', 'SPR.New.Existing', 'SchoolGradeType', 'DepartureMonth'])

    # Normalización de variables numéricas
    scaler = StandardScaler()
    df[['To.Grade', 'Is.Non.Annual.', 'Days', 'Tuition', 'FRP.Active', 'Total.Discount.Pax', 'School.Sponsor', 'FPP', 'NumberOfMeetingswithParents', 'SingleGradeTripFlag']] = scaler.fit_transform(df[['To.Grade', 'Is.Non.Annual.', 'Days', 'Tuition', 'FRP.Active', 'Total.Discount.Pax', 'School.Sponsor', 'FPP', 'NumberOfMeetingswithParents', 'SingleGradeTripFlag']])

    return df

# Assuming 'Retained.in.2012.' is the correct column name
df_new_preprocessed = df.drop('Retained.in.2012.', axis=1)

# Define the preprocess function
def preprocess(df):
    # Perform preprocessing steps on the DataFrame
    # ...

    # Return the preprocessed DataFrame
    return df

# Assuming 'Retained.in.2012.' is the correct column name
# Preprocess the DataFrame
df_new_preprocessed = preprocess(df)
X_new = df_new_preprocessed.drop('Retained.in.2012.', axis=1)
y_new = df_new_preprocessed['Retained.in.2012.']

X_array = X.values  # Convert DataFrame to numpy array
model.fit(X_array, y)

model = LogisticRegression(fit_intercept=False)
model.fit(X, y)

import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

X = df.drop('Retained.in.2012.', axis=1)
y = df['Retained.in.2012.']

# Scale the data
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# Create and train the model
model = LogisticRegression(max_iter=1000)
model.fit(X_train, y_train)

# Predict labels for the test set
y_pred = model.predict(X_test)

# Evaluate the model
print("Accuracy:", accuracy_score(y_test, y_pred))

# Assume df_new contains your new data
# Preprocess the new data
df_new_preprocessed = preprocess(df)  # Make sure to define the preprocess function
X_new = df.drop('Retained.in.2012.', axis=1)
y_new = df['Retained.in.2012.']

# Scale the new data
X_new_scaled = scaler.transform(X_new)

# Make predictions on the new data
y_new_pred = model.predict(X_new_scaled)

# Visualize the results
plt.figure(figsize=(10,6))
plt.scatter(range(len(y_new)), y_new, color='blue', label='Actual')
plt.scatter(range(len(y_new)), y_new_pred, color='red', label='Predicted')
plt.legend()
plt.show()

# Visualize the results
plt.figure(figsize=(10,6))
plt.scatter(range(len(y_new)), y_new, color='blue', label='Actual')
plt.scatter(range(len(y_new)), y_new_pred, color='red', label='Predicted')
plt.legend()
plt.xlabel('Client Index')
plt.ylabel('Retention')
plt.title('Actual vs. Predicted Retention for STC Clients')
plt.show()
